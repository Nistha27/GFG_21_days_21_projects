{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "title_markdown"
            },
            "source": [
                "# Day 4: AI in Healthcare - Building a Life-Saving Heart Disease Predictor\n",
                "\n",
                "## Project Overview\n",
                "In this project, we'll build a machine learning model to predict heart disease based on various medical attributes. This is a critical application of AI in healthcare that can help doctors make faster, more accurate diagnoses.\n",
                "\n",
                "## What You'll Learn\n",
                "- **Data Preprocessing**: Handle missing values, encode categorical variables, and scale features\n",
                "- **Exploratory Data Analysis**: Understand the relationships between medical attributes and heart disease\n",
                "- **Classification Models**: Build and compare multiple machine learning models\n",
                "- **Model Evaluation**: Use various metrics to assess model performance\n",
                "- **Feature Importance**: Identify which medical factors are most predictive of heart disease\n",
                "\n",
                "## Dataset\n",
                "We'll use the Heart Disease UCI dataset, which contains 14 attributes related to heart health:\n",
                "- **Age**: Age in years\n",
                "- **Sex**: Gender (1 = male, 0 = female)\n",
                "- **Chest Pain Type**: Type of chest pain experienced\n",
                "- **Resting Blood Pressure**: Blood pressure at rest\n",
                "- **Cholesterol**: Serum cholesterol level\n",
                "- **Fasting Blood Sugar**: Whether fasting blood sugar > 120 mg/dl\n",
                "- **Resting ECG**: Resting electrocardiographic results\n",
                "- **Max Heart Rate**: Maximum heart rate achieved\n",
                "- **Exercise Induced Angina**: Whether exercise induces angina\n",
                "- **ST Depression**: ST depression induced by exercise\n",
                "- **Slope**: Slope of peak exercise ST segment\n",
                "- **Number of Vessels**: Number of major vessels colored by fluoroscopy\n",
                "- **Thalassemia**: Blood disorder type\n",
                "- **Target**: Presence of heart disease (0 = no, 1 = yes)\n",
                "\n",
                "Let's start building our life-saving AI model!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_markdown"
            },
            "source": [
                "### Step 1: Import Libraries and Load Data\n",
                "First, let's import all the necessary libraries for data analysis and machine learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "setup_code"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "All libraries imported successfully!\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
                "\n",
                "# Set plot style\n",
                "sns.set_style('whitegrid')\n",
                "print(\"All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 276
                },
                "id": "load_data_code",
                "outputId": "25101a5e-dae2-4a32-c7d9-ec604d966cd5"
            },
            "outputs": [],
            "source": [
                "# Load the dataset from local file\n",
                "print(\"Loading Heart Disease dataset from local file...\")\n",
                "file_path = 'data/heart_disease_uci.csv'\n",
                "df = pd.read_csv(file_path)\n",
                "\n",
                "print(\"Dataset loaded successfully!\")\n",
                "print(f\"Data shape: {df.shape}\")\n",
                "print(f\"Columns: {list(df.columns)}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eda_markdown"
            },
            "source": [
                "### Step 2: Exploratory Data Analysis (EDA)\n",
                "Before building any models, we need to understand our data deeply. We'll look at the distribution of our target variable, the characteristics of our features, and how they relate to the presence of heart disease."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "initial_inspection_code",
                "outputId": "4ca5a903-67c8-4068-f4b9-54b6c84c7b59"
            },
            "outputs": [],
            "source": [
                "# Initial inspection\n",
                "print(\"Dataset Information:\")\n",
                "df.info()\n",
                "\n",
                "print(\"\\nDescriptive Statistics:\")\n",
                "print(df.describe())\n",
                "\n",
                "# Check for missing values\n",
                "print(\"\\nMissing Values:\")\n",
                "print(df.isnull().sum().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 586
                },
                "id": "c89E71xZgWnD",
                "outputId": "aaee826d-8f35-4c31-8536-75195a481956"
            },
            "outputs": [],
            "source": [
                "df.isnull().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "target_analysis_markdown"
            },
            "source": [
                "#### 2.1 Analyzing the Target Variable\n",
                "Let's see the distribution of patients with and without heart disease."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 564
                },
                "id": "target_analysis_code",
                "outputId": "2d84e9a0-5fac-4c32-a0d1-457b76a5ad7f"
            },
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 6))\n",
                "sns.countplot(x='num', data=df, palette='viridis', hue='num', legend=False)\n",
                "plt.title('Distribution of Heart Disease (1 = Disease, 0 = No Disease)')\n",
                "plt.xlabel('Target')\n",
                "plt.ylabel('Count')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "target_analysis_summary"
            },
            "source": [
                "**Insight:** The dataset is fairly balanced, with a slightly higher number of patients having heart disease. This is good because it means our model will have a similar number of examples for both classes to learn from, and accuracy will be a meaningful metric."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "feature_analysis_markdown"
            },
            "source": [
                "#### 2.2 Analyzing Features vs. Target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "feature_analysis_code",
                "outputId": "00050dd7-a468-4c98-ca7f-a024c848ad21"
            },
            "outputs": [],
            "source": [
                "# Let's visualize the relationship between key features and the target\n",
                "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
                "fig.suptitle('Key Features vs. Heart Disease', fontsize=16)\n",
                "\n",
                "# Age vs. Target\n",
                "sns.histplot(ax=axes[0, 0], data=df, x='age', hue='num', multiple='stack', palette='plasma').set_title('Age Distribution by Target')\n",
                "\n",
                "# Max Heart Rate vs. Target\n",
                "sns.boxplot(ax=axes[0, 1], data=df, x='num', y='thalch', palette='magma', hue='num', legend=False).set_title('Max Heart Rate by Target')\n",
                "\n",
                "# Chest Pain Type vs. Target\n",
                "cp_plot = sns.countplot(ax=axes[1, 0], data=df, x='cp', hue='num', palette='cividis')\n",
                "cp_plot.set_title('Chest Pain Type by Target')\n",
                "cp_plot.set_xticks(range(len(df['cp'].unique())))\n",
                "cp_plot.set_xticklabels(['Typical Angina', 'Atypical Angina', 'Non-anginal Pain', 'Asymptomatic'])\n",
                "\n",
                "# Sex vs. Target\n",
                "sex_plot = sns.countplot(ax=axes[1, 1], data=df, x='sex', hue='num', palette='inferno')\n",
                "sex_plot.set_title('Sex by Target')\n",
                "sex_plot.set_xticks(range(len(df['sex'].unique())))\n",
                "sex_plot.set_xticklabels(['Female', 'Male'])\n",
                "\n",
                "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "feature_analysis_summary"
            },
            "source": [
                "**Insights:**\n",
                "- **Max Heart Rate (`thalach`):** Patients with heart disease tend to have a lower maximum heart rate.\n",
                "- **Chest Pain (`cp`):** Patients with chest pain types 1 and 2 (Atypical and Non-anginal) are more likely to have heart disease. Surprisingly, those with type 0 (Typical Angina) are less likely, and those with asymptomatic pain (type 3) are very likely to have the disease.\n",
                "- **Sex:** A higher proportion of females in this dataset have heart disease compared to males."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "correlation_code",
                "outputId": "d4592b1e-3d14-4230-cac4-97b2bb88447f"
            },
            "outputs": [],
            "source": [
                "# Correlation Heatmap\n",
                "plt.figure(figsize=(16, 12))\n",
                "# Select only numerical columns for correlation calculation\n",
                "numerical_df = df.select_dtypes(include=np.number)\n",
                "sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
                "plt.title('Correlation Matrix of Numerical Features')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "preprocessing_markdown"
            },
            "source": [
                "### Step 3: Data Preprocessing\n",
                "Even though the data is clean, we need to prepare it for our models. This involves:\n",
                "1.  **Separating features (X) and target (y).**\n",
                "2.  **Identifying categorical features** that need to be encoded.\n",
                "3.  **One-Hot Encoding** categorical features to convert them into a numerical format.\n",
                "4.  **Scaling numerical features** so they are on a similar scale."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HjVVXv-veLpM"
            },
            "source": [
                "## **Theoretical Concept: Scikit-Learn Pipelines**\n",
                "\n",
                "A **Pipeline** in Scikit-Learn is a way to automate a machine learning workflow. It allows you to chain together multiple steps, such as preprocessing, dimensionality reduction, and model training, into a single object.\n",
                "\n",
                "**Why use Pipelines?**\n",
                "\n",
                "1.  **Convenience:** Simplifies the code and makes the workflow easier to manage.\n",
                "2.  **Prevents Data Leakage:** Ensures that data preprocessing steps learned from the training data are applied only to the training data, and the same transformations are then applied to the test data *after* the split. This prevents information from the test set from \"leaking\" into the training process.\n",
                "3.  **Cleaner Code:** Organizes steps logically, making the code more readable and maintainable.\n",
                "4.  **Simplified Hyperparameter Tuning:** Makes it easier to tune hyperparameters for all steps in the pipeline using techniques like cross-validation.\n",
                "\n",
                "In this project, we'll use a pipeline to combine our preprocessing steps (imputation, scaling, and one-hot encoding) with our classification models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "split_data_code"
            },
            "outputs": [],
            "source": [
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "# Define features (X) and target (y)\n",
                "X = df.drop('num', axis=1)\n",
                "y = df['num']\n",
                "\n",
                "# Drop the 'id' and 'dataset' columns as they are not features\n",
                "X = X.drop(['id', 'dataset'], axis=1)\n",
                "\n",
                "\n",
                "# Identify categorical and numerical features\n",
                "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
                "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
                "\n",
                "# Create preprocessing pipelines for numerical and categorical features\n",
                "numerical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='mean')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')), # Added imputation for categorical features\n",
                "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "# Create a column transformer to apply different transformations to different columns\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, numerical_features),\n",
                "        ('cat', categorical_transformer, categorical_features)])\n",
                "\n",
                "# Split data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Rb_mK8CvEWaA"
            },
            "source": [
                "* Create numerical preprocessing pipeline: A Pipeline is created to handle numerical features. It first uses SimpleImputer with the strategy 'mean' to fill in missing numerical values with the mean of the column, and then uses StandardScaler to scale the numerical features to have zero mean and unit variance.\n",
                "* Create categorical preprocessing pipeline: A Pipeline is created for categorical features. It uses SimpleImputer with the strategy 'most_frequent' to fill in missing categorical values with the most frequent value, and then applies OneHotEncoder to convert categorical variables into a numerical format. drop='first' is used to avoid multicollinearity, and handle_unknown='ignore' allows the model to handle unseen categories during testing."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}