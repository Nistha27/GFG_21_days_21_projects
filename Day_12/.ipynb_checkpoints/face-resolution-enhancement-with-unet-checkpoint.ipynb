{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Super-Resolution using U-Net\n",
    "\n",
    "This notebook demonstrates how to build and train a U-Net model for image super-resolution.  \n",
    "The goal is to take a low-resolution image (64x64) and generate a high-resolution version (128x128).\n",
    "\n",
    "The U-Net architecture is well-suited for this task as it effectively captures both local and global features through its encoder-decoder structure with skip connections.\n",
    "\n",
    "![Super Resolution Example](https://raw.githubusercontent.com/AshishJangra27/ai-projects/refs/heads/main/Image%20Enhancement%20with%20U-Net/img.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "\n",
    "This cell imports all the necessary libraries for building and training the U-Net model, including Keras for model definition, OpenCV for image processing, and Matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, GlobalMaxPooling2D, concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. U-Net Model Definition\n",
    "\n",
    "This cell defines the U-Net model architecture for image super-resolution. The `unet_64to128` function creates a U-Net model that takes a 64x64x3 image as input and outputs a 128x128x3 image. It includes an encoder, a bottleneck, and a decoder with skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def unet_64to128(input_shape=(64, 64, 3), n_classes=3, final_activation='sigmoid', dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape, name='img')\n",
    "\n",
    "    # Encoder\n",
    "    c1 = Conv2D(16, (3,3), padding='same')(inputs)\n",
    "    c1 = BatchNormalization()(c1); c1 = Activation('relu')(c1)\n",
    "    c1 = Conv2D(16, (3,3), padding='same')(c1)\n",
    "    c1 = BatchNormalization()(c1); c1 = Activation('relu')(c1)\n",
    "    p1 = MaxPooling2D((2,2))(c1); p1 = Dropout(dropout_rate)(p1)   # 64 -> 32\n",
    "\n",
    "    c2 = Conv2D(32, (3,3), padding='same')(p1)\n",
    "    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n",
    "    c2 = Conv2D(32, (3,3), padding='same')(c2)\n",
    "    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n",
    "    p2 = MaxPooling2D((2,2))(c2); p2 = Dropout(dropout_rate)(p2)   # 32 -> 16\n",
    "\n",
    "    c3 = Conv2D(64, (3,3), padding='same')(p2)\n",
    "    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n",
    "    c3 = Conv2D(64, (3,3), padding='same')(c3)\n",
    "    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n",
    "    p3 = MaxPooling2D((2,2))(c3); p3 = Dropout(dropout_rate)(p3)   # 16 -> 8\n",
    "\n",
    "    c4 = Conv2D(128, (3,3), padding='same')(p3)\n",
    "    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n",
    "    c4 = Conv2D(128, (3,3), padding='same')(c4)\n",
    "    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n",
    "    p4 = MaxPooling2D((2,2))(c4); p4 = Dropout(dropout_rate)(p4)   # 8 -> 4\n",
    "\n",
    "    # Bottleneck (4x4)\n",
    "    c5 = Conv2D(256, (3,3), padding='same')(p4)\n",
    "    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n",
    "    c5 = Conv2D(256, (3,3), padding='same')(c5)\n",
    "    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n",
    "\n",
    "    # Decoder (back to 64x64)\n",
    "    u6 = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same')(c5)  # 4 -> 8\n",
    "    u6 = concatenate([u6, c4]); u6 = Dropout(dropout_rate)(u6)\n",
    "    u6 = Conv2D(128, (3,3), padding='same')(u6)\n",
    "    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n",
    "    u6 = Conv2D(128, (3,3), padding='same')(u6)\n",
    "    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same')(u6)    # 8 -> 16\n",
    "    u7 = concatenate([u7, c3]); u7 = Dropout(dropout_rate)(u7)\n",
    "    u7 = Conv2D(64, (3,3), padding='same')(u7)\n",
    "    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n",
    "    u7 = Conv2D(64, (3,3), padding='same')(u7)\n",
    "    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (3,3), strides=(2,2), padding='same')(u7)    # 16 -> 32\n",
    "    u8 = concatenate([u8, c2]); u8 = Dropout(dropout_rate)(u8)\n",
    "    u8 = Conv2D(32, (3,3), padding='same')(u8)\n",
    "    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n",
    "    u8 = Conv2D(32, (3,3), padding='same')(u8)\n",
    "    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u8)    # 32 -> 64\n",
    "    u9 = concatenate([u9, c1]); u9 = Dropout(dropout_rate)(u9)\n",
    "    u9 = Conv2D(16, (3,3), padding='same')(u9)\n",
    "    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n",
    "    u9 = Conv2D(16, (3,3), padding='same')(u9)\n",
    "    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n",
    "\n",
    "    # Extra upsample to reach 128x128 (no skip)\n",
    "    u10 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u9)   # 64 -> 128\n",
    "    u10 = Dropout(dropout_rate)(u10)\n",
    "    u10 = Conv2D(16, (3,3), padding='same')(u10)\n",
    "    u10 = BatchNormalization()(u10); u10 = Activation('relu')(u10)\n",
    "\n",
    "    outputs = Conv2D(n_classes, (1,1), activation=final_activation, name='mask')(u10)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs, name='UNet_64to128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Initialization and Compilation\n",
    "\n",
    "This cell initializes the U-Net model with the specified input shape and number of classes. It then compiles the model using the Adam optimizer and mean absolute error (MAE) as the loss function. The input and output shapes are printed to verify the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 64, 64, 3)\n",
      "Output shape: (None, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "model = unet_64to128(input_shape=(64,64,3), n_classes=3, final_activation='sigmoid')\n",
    "\n",
    "print('Input shape:', model.input_shape)   # (None, 64, 64, 3)\n",
    "print('Output shape:', model.output_shape) # (None, 128, 128, 3)\n",
    "\n",
    "model.compile(optimizer=Adam(1e-4), loss='mae' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Generator\n",
    "\n",
    "This cell defines a data generator function `datagen` that loads and preprocesses images from the CelebA dataset. It resizes the images to 128x128 as ground truth and to 64x64 as low-resolution input, and normalizes the pixel values. The generator yields batches of low-resolution and high-resolution image pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m imgs = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdatagen\u001b[39m(batch_size):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "imgs = os.listdir('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/')\n",
    "\n",
    "\n",
    "def datagen(batch_size):\n",
    "    \n",
    "    while True:\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            indx = np.random.randint(0, len(imgs))\n",
    "            \n",
    "            bgr = cv.imread('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/' + imgs[indx])\n",
    "            bgr = cv.resize(bgr, (128, 128))\n",
    "            rgb = cv.cvtColor(bgr, cv.COLOR_BGR2RGB)\n",
    "\n",
    "            # blur = cv.blur(rgb, (4, 4))\n",
    "\n",
    "            x = cv.resize(rgb, (64, 64))\n",
    "            x = x / 255.0\n",
    "            y = rgb / 255.0\n",
    "\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "        \n",
    "        x_batch = np.array(x_batch).reshape(batch_size, 64, 64, 3)\n",
    "        y_batch = np.array(y_batch).reshape(batch_size, 128, 128, 3)\n",
    "        \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training (Short Run)\n",
    "\n",
    "This cell trains the U-Net model for a small number of epochs (5) using the `datagen` function. This is a short run to quickly check if the model is training without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "batch_size = 32\n",
    "num_images = len(imgs)\n",
    "steps_per_epoch = num_images // batch_size \n",
    "\n",
    "\n",
    "results = model.fit(datagen(batch_size=batch_size), steps_per_epoch=steps_per_epoch , epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Training with Callbacks and Sample Saving\n",
    "\n",
    "This cell trains the U-Net model for a longer duration (3 epochs) and includes custom callbacks. The `show_and_save_samples` function generates and saves sample super-resolved images at the end of each epoch, along with the original, ground truth, and low-resolution images for comparison. The `LambdaCallback` is used to execute this function after each epoch. The model checkpoints are also saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2 as cv, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n",
    "\n",
    "def show_and_save_samples(model, imgs, base_dir, epoch, n=5, save_dir='./epoch_samples'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    chosen = np.random.choice(imgs, n, replace=False)\n",
    "    fig, axes = plt.subplots(n, 4, figsize=(16, 4*n))\n",
    "    if n == 1: axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, fname in enumerate(chosen):\n",
    "        path = os.path.join(base_dir, fname)\n",
    "        rgb = cv.cvtColor(cv.imread(path), cv.COLOR_BGR2RGB)\n",
    "        gt128 = cv.resize(rgb, (128,128)).astype('float32')/255\n",
    "        img64 = cv.resize(rgb, (64,64)).astype('float32')/255\n",
    "        pred  = model.predict(np.expand_dims(img64,0), verbose=0)[0]\n",
    "        lowup = cv.resize((img64*255).astype(np.uint8),(128,128))\n",
    "\n",
    "        for ax, im, title in zip(axes[i],[rgb,gt128,lowup,pred],\n",
    "                                 ['Original','GT 128x128','Low-Res','Prediction']):\n",
    "            ax.imshow(im); ax.set_title(title); ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(); plt.savefig(f\"{save_dir}/epoch_{epoch+1:02d}.png\"); plt.show()\n",
    "\n",
    "# --- Simple Callbacks ---\n",
    "def on_epoch_end(epoch, logs):\n",
    "    show_and_save_samples(model, imgs, base_dir, epoch, n=5)\n",
    "    model.save(f'./checkpoints/model_epoch_{epoch+1:02d}.keras')\n",
    "\n",
    "show_cb = tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(imgs)//batch_size\n",
    "base_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "results = model.fit(\n",
    "    datagen(batch_size=batch_size),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=3,\n",
    "    callbacks=[show_cb],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Project Summary\n",
    "\n",
    "This notebook successfully implemented and trained a U-Net model for image super-resolution. The model was trained on the CelebA dataset, taking 64x64 images as input and generating 128x128 images.\n",
    "\n",
    "The training process showed a decrease in the mean absolute error (MAE) loss over epochs, indicating that the model is learning to reconstruct higher-resolution images. The generated sample images at the end of each epoch provide a visual representation of the model's progress.\n",
    "\n",
    "Further improvements could include:\n",
    "- Experimenting with different loss functions (e.g., perceptual loss)\n",
    "- Using a larger and more diverse dataset\n",
    "- Implementing more advanced super-resolution techniques (e.g., Generative Adversarial Networks)\n",
    "- Hyperparameter tuning to optimize model performance"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29962,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML Environment (Python 3.11 + TensorFlow)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
